{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bad32605",
   "metadata": {},
   "source": [
    "# RAG Pipeline Exercise\n",
    "\n",
    "In this exercise you will build and **compare two simple Retrieval-Augmented Generation (RAG) pipelines**.\n",
    "\n",
    "You will work with a small collection of PDF documents (e.g. medical guidelines) and:\n",
    "\n",
    "1. Load and chunk the PDF documents.\n",
    "2. Create a vector index using **embedding model A** (local `BAAI/bge-m3`).\n",
    "3. Create a second index using **embedding model B** (e.g. OpenAI or Gemini embeddings).\n",
    "4. Implement a simple **retriever** and an **answering function** that calls an LLM with retrieved context.\n",
    "5. Automatically **generate questions** from the documents and use them to **compare two RAG configurations**.\n",
    "\n",
    "Cells marked with `# TODO` are **for students to implement**.\n",
    "Everything else is provided scaffolding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddf82e6",
   "metadata": {},
   "source": [
    "## 0. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be93ec53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# TODO (easy): skim the imports and make sure you understand what each library is used for.\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import glob\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# LLM / API clients (we will mainly use OpenAI here; Gemini can be added as a bonus)\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68f16980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load API keys from .env (you need to create this file once and add your keys)\n",
    "load_dotenv()\n",
    "\n",
    "deepinfra_key = os.getenv(\"DEEPINFRA_API_KEY\")\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "anthropic_api_key = os.getenv(\"ANTHROPIC_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba2719d",
   "metadata": {},
   "source": [
    "## 1. Load PDF documents\n",
    "\n",
    "We assume there is a `data/` folder containing one or more PDF files.\n",
    "\n",
    "**Task:** implement `load_pdfs(glob_path)` so that it:\n",
    "- Iterates over all PDF files matching `glob_path`\n",
    "- Reads them with `PdfReader`\n",
    "- Concatenates the text of all pages into **one long string**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8abcb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdfs(glob_path: str = \"data/*.pdf\") -> str:\n",
    "    \"\"\"Load all PDFs matching the pattern and return their combined text.\n",
    "\n",
    "    TODO:\n",
    "    - Use `glob.glob(glob_path)` to iterate over file paths\n",
    "    - For each file, open it in binary mode and create a `PdfReader`\n",
    "    - Loop over `reader.pages` and extract text with the extract_text() function\n",
    "    - Concatenate everything into a single string `text`\n",
    "    - Be robust: skip pages where `extract_text()` returns None\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    text = \"\"\n",
    "    for pdf_path in glob.glob(glob_path):\n",
    "        with open(pdf_path, \"rb\") as f:\n",
    "            reader = PdfReader(f)\n",
    "            for page in reader.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    text += \" \" + page_text\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "416c3ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters: 230708\n",
      "Preview:  Asthma: diagnosis, \n",
      "moni toring and chr onic \n",
      "asthma manag emen t (BTS, \n",
      "NICE, SI GN) \n",
      "NICE guideline \n",
      "Published: 27 No vember 202 4 \n",
      "www .nice.or g.uk/guidance/ng2 45 \n",
      "© NICE 202 4. All right s reserved. Subject t o Notice of right s (https://www .nice.or g.uk/t erms-and-\n",
      "conditions#notice-of -right s). Your r esponsi bility \n",
      "The r ecommendations in t his guideline r epresent t he view of NICE, arriv ed at aft er car eful \n",
      "consideration of t he evidence a vailable. When e xercising t heir judg\n"
     ]
    }
   ],
   "source": [
    "# Run once and inspect\n",
    "raw_text = load_pdfs(\"data/*.pdf\")\n",
    "print(\"Number of characters:\", len(raw_text))\n",
    "print(\"Preview:\", raw_text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0c5a14",
   "metadata": {},
   "source": [
    "## 2. Chunk the text\n",
    "\n",
    "We will split the long text into overlapping chunks.\n",
    "\n",
    "Later you can **experiment** with different `chunk_size` and `chunk_overlap` to see how it affects retrieval.\n",
    "\n",
    "**Task:** start with the given parameters, run once, then try at least one alternative configuration and note the effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4e3f802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG A: 130 chunks produced, first chunk length = 1995\n",
      "RAG B: 260 chunks produced, first chunk length = 979\n"
     ]
    }
   ],
   "source": [
    "# Base configuration (RAG A)\n",
    "chunk_size_a = 2000\n",
    "chunk_overlap_a = 200\n",
    "\n",
    "splitter_a = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size_a,\n",
    "    chunk_overlap=chunk_overlap_a\n",
    ")\n",
    "\n",
    "chunks_a = splitter_a.split_text(raw_text)\n",
    "print(f\"RAG A: {len(chunks_a)} chunks produced, first chunk length = {len(chunks_a[0])}\")\n",
    "\n",
    "# TODO (mini-experiment): change chunk_size / chunk_overlap for RAG B and compare\n",
    "chunk_size_b = 1000   # e.g. smaller chunks\n",
    "chunk_overlap_b = 100\n",
    "\n",
    "splitter_b = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size_b,\n",
    "    chunk_overlap=chunk_overlap_b\n",
    ")\n",
    "\n",
    "chunks_b = splitter_b.split_text(raw_text)\n",
    "print(f\"RAG B: {len(chunks_b)} chunks produced, first chunk length = {len(chunks_b[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3511f4",
   "metadata": {},
   "source": [
    "## 3. Create embeddings and a FAISS index\n",
    "\n",
    "We start with **Embedding model A: `BAAI/bge-small-en`** using `sentence-transformers`.\n",
    "\n",
    "Then, as an optional extension, you can build **Embedding model B** using OpenAI or Gemini and compare.\n",
    "\n",
    "To keep the exercise manageable, the base version only **requires** BGE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b519161f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dimensionality (A): 1024\n",
      "FAISS index (A) size: 130\n"
     ]
    }
   ],
   "source": [
    "# Embedding model A (local)\n",
    "model_name_a = \"BAAI/bge-small-en\"\n",
    "embedder_a = SentenceTransformer(model_name_a)\n",
    "\n",
    "# Compute embeddings for all chunks of configuration A\n",
    "embeddings_a = embedder_a.encode(chunks_a, convert_to_numpy=True)\n",
    "\n",
    "dimensions_a = embeddings_a.shape[1]\n",
    "print(\"Embedding dimensionality (A):\", dimensions_a)\n",
    "\n",
    "index_a = faiss.IndexFlatL2(dimensions_a)\n",
    "index_a.add(embeddings_a)\n",
    "print(\"FAISS index (A) size:\", index_a.ntotal)\n",
    "\n",
    "# Persist index/chunks if you like (optional)\n",
    "os.makedirs(\"faiss\", exist_ok=True)\n",
    "faiss.write_index(index_a, \"faiss/faiss_index_a.index\")\n",
    "with open(\"faiss/chunks_a.pkl\", \"wb\") as f:\n",
    "    pickle.dump(chunks_a, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77271916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index (B) size: 260\n"
     ]
    }
   ],
   "source": [
    "# OPTIONAL / BONUS: Embedding model B using OpenAI embeddings.\n",
    "\n",
    "# TODO (bonus):\n",
    "# - Use `openai_client.embeddings.create(...)` to compute embeddings for `chunks_b`\n",
    "# - Create a second FAISS index `index_b`\n",
    "# - Make sure to check the dimensionality from the first embedding vector\n",
    "\n",
    "# Example sketch (not complete, adapt & run if you have API access):\n",
    "# Initialize OpenAI client\n",
    "openai_client = OpenAI(api_key=openai_api_key)\n",
    "response = openai_client.embeddings.create(\n",
    "     model=\"text-embedding-3-small\",\n",
    "    input=chunks_b\n",
    ")\n",
    "embeddings_b = np.array([item.embedding for item in response.data])\n",
    "dim_b = embeddings_b.shape[1]\n",
    "index_b = faiss.IndexFlatL2(dim_b)\n",
    "index_b.add(embeddings_b)\n",
    "print(\"FAISS index (B) size:\", index_b.ntotal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0339224e",
   "metadata": {},
   "source": [
    "## 4. Implement a simple retriever\n",
    "\n",
    "We now implement a generic retrieval function that:\n",
    "1. Embeds the query.\n",
    "2. Searches the FAISS index.\n",
    "3. Returns the corresponding text chunks.\n",
    "\n",
    "We implement it for configuration A. If you built configuration B, you can reuse the same function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa6fbf82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of retrieved chunks: 3\n",
      "Preview of first chunk: suggestiv e of ast hma. \n",
      "Full details of t he evidence and t he committ ee's discussion ar e in: \n",
      "• evidence r eview  A: diagnostic t est accuracy of spir ometr y in people suspect ed of \n",
      "asthma \n",
      "• evidence r eview  B: diagnostic t est accuracy f or br onchodilat or reversibility in \n",
      "people suspect ed of ast hma \n",
      "• evidence r eview  C: diagnostic t est accuracy of peak e xpirat ory flow variabilit\n"
     ]
    }
   ],
   "source": [
    "def retrieve_texts(query: str, k: int, index, chunks, embedder) -> list:\n",
    "    \"\"\"Return the top-k most similar chunks for a query.\n",
    "\n",
    "    TODO (students):\n",
    "    - Encode the query with `embedder.encode(...)`\n",
    "    - Call `index.search(query_embedding, k)`\n",
    "    - Use the returned indices to select the chunks\n",
    "    - Return a list of strings (chunks)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    query_emb = embedder.encode([query], convert_to_numpy=True)\n",
    "    distances, indices = index.search(query_emb, k)\n",
    "    retrieved = [chunks[i] for i in indices[0]]\n",
    "    return retrieved\n",
    "\n",
    "# Quick sanity check\n",
    "test_query = \"What is the most important factor in diagnosing asthma?\"\n",
    "retrieved_test = retrieve_texts(test_query, k=3, index=index_a, chunks=chunks_a, embedder=embedder_a)\n",
    "print(\"Number of retrieved chunks:\", len(retrieved_test))\n",
    "print(\"Preview of first chunk:\", retrieved_test[0][:400])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f646fdc6",
   "metadata": {},
   "source": [
    "## 5. Implement `answer_query` using an LLM\n",
    "\n",
    "Now we build the actual RAG call:\n",
    "\n",
    "1. Use `retrieve_texts` to get top-`k` chunks.\n",
    "2. Concatenate them into a context string.\n",
    "3. Build a prompt that:\n",
    "   - shows the context\n",
    "   - asks the model to answer the user question based **only** on this context.\n",
    "4. Call the OpenAI chat completion API.\n",
    "\n",
    "This is the **core RAG function**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d94610c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG answer: The most important factor in diagnosing asthma, especially in children under 5, is differentiating asthma from symptoms caused by recurrent viral infections.\n"
     ]
    }
   ],
   "source": [
    "def answer_query(query: str, k: int, index, chunks, embedder, client: OpenAI) -> str:\n",
    "    \"\"\"RAG-style answer: retrieve context and ask an LLM.\n",
    "\n",
    "    TODO (students):\n",
    "    - Use `retrieve_texts` to get `k` relevant chunks.\n",
    "    - Join them into a single context string.\n",
    "    - Build a chat prompt that instructs the model to answer *only* using the context.\n",
    "    - Call `client.chat.completions.create(...)` with model `\"gpt-4o-mini\"` (or similar).\n",
    "    - Return the model's answer text.\n",
    "    \"\"\"\n",
    "    retrieved_chunks = retrieve_texts(query, k, index, chunks, embedder)\n",
    "    context = \"\\n\\n---\\n\\n\".join(retrieved_chunks)\n",
    "\n",
    "    system_prompt = (\n",
    "        \"You are a helpful assistant answering questions based ONLY on the provided context. \"\n",
    "        \"If the answer is not in the context, say that you do not know.\"\n",
    "    )\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": f\"Context:\\n{context}\\n\\nQuestion: {query}\"}\n",
    "    ]\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message.content.strip()\n",
    "\n",
    "# Quick manual test\n",
    "answer = answer_query(test_query, k=3, index=index_a, chunks=chunks_a, embedder=embedder_a, client=openai_client)\n",
    "print(\"RAG answer:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a86a92e",
   "metadata": {},
   "source": [
    "## 6. Generate questions from random chunks (automatic evaluation set)\n",
    "\n",
    "To compare two RAG configurations, we need **questions**.\n",
    "\n",
    "We will:\n",
    "- randomly sample a few chunks from the corpus,\n",
    "- ask an LLM to generate a **good question** whose answer is contained in the chunk.\n",
    "\n",
    "Then we can use these question–chunk pairs as a small evaluation set.\n",
    "\n",
    "We provide most of the implementation. Your job is mainly to:\n",
    "- inspect the code,\n",
    "- understand the prompt,\n",
    "- maybe tweak the number of chunks or retries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb899fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1: What criteria should be used to determine whether a patient with high blood pressure requires same-day referral for specialist assessment, based on the 2019 recommendations?\n",
      "  From chunk preview: higher), but no sympt oms or signs indicating same-da y referral (see \n",
      "recommendation 1 .5.2), carr y out in vestigation...\n",
      "\n",
      "Q2: What are the implications of the BTS/NICE/SIGN recommendations on the use of different asthma treatment regimens for children and adults, and how do they address issues of cost-effectiveness and feasibility in diagnosis and management?\n",
      "  From chunk preview: (ICS) r egimens (using SABA [shor t-acting beta 2 agonist] as a r eliever) compar ed wit h 'as-\n",
      "needed' strat egies (f o...\n",
      "\n",
      "Q3: What are the key strategies recommended for improving asthma management and patient self-management education in primary care, according to the NICE guidelines?\n",
      "  From chunk preview: sympt oms do not impr ove. \n",
      "When incr easing ICS tr eatment: \n",
      "• consider quadrupling t he regular ICS dose \n",
      "• do not e x...\n",
      "\n",
      "Q4: What evidence led the committee to recommend low-dose MART as the optimal initial treatment for children with newly diagnosed asthma, and how might this recommendation influence clinical practice?\n",
      "  From chunk preview: they made a resear ch recommendation t o test t he benefit s of t his combination in childr en. \n",
      "How the r ecommenda tio...\n",
      "\n",
      "Q5: What are the recommendations in the NICE guidelines for managing hypertension in women of childbearing potential, particularly regarding treatment during pregnancy and breastfeeding?\n",
      "  From chunk preview: related clarification on br eastf eeding . \n",
      "1.4.26 For guidance on choice of antih yper tensiv e medicine in people wit ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_questions_for_random_chunks(chunks, num_chunks: int = 5, max_retries: int = 2):\n",
    "    selected_chunks = random.sample(chunks, num_chunks)\n",
    "    qa_pairs = []\n",
    "\n",
    "    for chunk in selected_chunks:\n",
    "        prompt = prompt = (\n",
    "            \"Based on the following text, generate an insightful question that covers its key content:\\n\\n\"\n",
    "            \"Text:\\n\" + chunk + \"\\n\\n\"\n",
    "            \"Question:\"\n",
    "        )\n",
    "\n",
    "        question = None\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                completion = openai_client.chat.completions.create(\n",
    "                    model=\"gpt-4o-mini\",\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "                )\n",
    "                question = completion.choices[0].message.content.strip()\n",
    "                if question:\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                print(\"Error while generating question, retrying...\", e)\n",
    "\n",
    "        if question is None:\n",
    "            question = \"Error: could not generate question.\"\n",
    "\n",
    "        qa_pairs.append((chunk, question))\n",
    "\n",
    "    return qa_pairs\n",
    "\n",
    "questions = generate_questions_for_random_chunks(chunks_a, num_chunks=5, max_retries=2)\n",
    "for i, (chunk, q) in enumerate(questions, 1):\n",
    "    print(f\"Q{i}: {q}\\n  From chunk preview: {chunk[:120]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd0eaf5",
   "metadata": {},
   "source": [
    "## 7. Compare two RAG configurations\n",
    "\n",
    "Now we can:\n",
    "- Use the generated questions,\n",
    "- Answer them with RAG configuration A (BGE + chunking A),\n",
    "- (Optional) Answer them with RAG configuration B (e.g. different chunking and/or different embeddings),\n",
    "- Compare the answers qualitatively.\n",
    "\n",
    "To keep the exercise manageable, we start with config A only.\n",
    "If you implemented config B, reuse `answer_query` with `index_b`, `chunks_b`, and your second embedder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a292474a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What criteria should be used to determine whether a patient with high blood pressure requires same-day referral for specialist assessment, based on the 2019 recommendations?\n",
      "Answer A: A patient with high blood pressure requires same-day referral for specialist assessment if they have a clinic blood pressure of 180/120 mmHg or higher, along with either of the following criteria:\n",
      "\n",
      "1. Signs of retinal haemorrhage or papilloedema (accelerated hypertension).\n",
      "2. Life-threatening symptoms such as new onset confusion, chest pain, signs of heart failure, or acute kidney injury. \n",
      "\n",
      "Additionally, if there is suspected phaeochromocytoma (for example, labile or postural hypotension, headache, palpitations, pallor, abdominal pain, or diaphoresis), same-day referral is also warranted.\n",
      "Source chunk preview: higher), but no sympt oms or signs indicating same-da y referral (see \n",
      "recommendation 1 .5.2), carr y out in vestigations f or tar get or gan damage ( ...\n",
      "------------------------------------------------------------\n",
      "Question: What are the implications of the BTS/NICE/SIGN recommendations on the use of different asthma treatment regimens for children and adults, and how do they address issues of cost-effectiveness and feasibility in diagnosis and management?\n",
      "Answer A: The implications of the BTS/NICE/SIGN recommendations on the use of different asthma treatment regimens for children and adults include the need for updated approaches to asthma management, focusing on personalized treatment strategies based on the severity and specifics of an individual's asthma. The guidelines emphasize the importance of accurate diagnosis and effective management to reduce asthma attacks and improve overall control.\n",
      "\n",
      "Specifically for children aged 5 to 11, there is a recommendation for research comparing inhaled corticosteroid (ICS) regimens with short-acting beta-2 agonist (SABA) as relievers against \"as-needed\" strategies, indicating a careful consideration of the most effective and suitable initial treatment.\n",
      "\n",
      "For adults and individuals aged 12 and over whose asthma is not controlled on ICS plus formoterol used as needed, the guidelines also call for research into the best step-up treatment options.\n",
      "\n",
      "Regarding cost-effectiveness and feasibility, one of the recommendations is to evaluate the cost-effectiveness and feasibility of the proposed diagnostic pathways for asthma in children, young people, and adults. The inclusion of digital inhaler monitors is proposed to assess their potential to improve adherence to preventer inhalers, thereby impacting asthma control positively.\n",
      "\n",
      "Overall, the recommendations aim to enhance the management of asthma through evidence-based strategies that are both practical and economically viable, with an emphasis on improving the quality of care for patients at all ages.\n",
      "Source chunk preview: (ICS) r egimens (using SABA [shor t-acting beta 2 agonist] as a r eliever) compar ed wit h 'as-\n",
      "needed' strat egies (f or example ICS/f ormot erol) as ...\n",
      "------------------------------------------------------------\n",
      "Question: What are the key strategies recommended for improving asthma management and patient self-management education in primary care, according to the NICE guidelines?\n",
      "Answer A: The key strategies recommended for improving asthma management and patient self-management education in primary care according to the NICE guidelines include:\n",
      "\n",
      "1. Proactive alerts to ensure routine reviews.\n",
      "2. Structured protocols for asthma reviews.\n",
      "3. Support from primary care and community pharmacists.\n",
      "4. Mailing or emailing of educational resources.\n",
      "5. Telephone calls to provide ongoing support and advice.\n",
      "6. IT-based education and monitoring.\n",
      "7. Involvement of community workers to support clinical teams in deprived and/or ethnic minority communities.\n",
      "8. Educational programs provided in schools by appropriately trained personnel.\n",
      "9. Inclusion of a written personalized action plan that incorporates approaches to minimizing exposure to indoor and outdoor air pollution.\n",
      "10. Regular reviews at significant events such as hospital admissions or annual reviews. \n",
      "\n",
      "These strategies aim to enhance patient engagement and ensure that care is tailored to individual needs.\n",
      "Source chunk preview: sympt oms do not impr ove. \n",
      "When incr easing ICS tr eatment: \n",
      "• consider quadrupling t he regular ICS dose \n",
      "• do not e xceed t he maximum licensed dai ...\n",
      "------------------------------------------------------------\n",
      "Question: What evidence led the committee to recommend low-dose MART as the optimal initial treatment for children with newly diagnosed asthma, and how might this recommendation influence clinical practice?\n",
      "Answer A: The committee recommended regular low-dose ICS plus SABA as needed as the initial treatment for children diagnosed with asthma based on several studies that directly addressed the question of optimal add-on therapy. The evidence for MART was derived from a single study showing that MART was superior to both regular moderate-dose ICS plus SABA as needed and regular low-dose ICS/L ABA plus SABA as needed. Specifically, it reduced the number of exacerbations, decreased the need for reliever inhalers, and caused fewer adverse events. Additionally, the economic analysis supported the clinical evidence, indicating that MART was associated with fewer costs and more quality-adjusted life years (QALYs).\n",
      "\n",
      "In terms of clinical practice, the recommendation is expected to lead to more patients being switched to MART than to other treatment options. However, since MART is already in use, this change should not be disruptive. Overall, the recommendations aim to improve asthma management in children, potentially reducing asthma attacks and enhancing the quality of care while remaining cost-effective for the NHS.\n",
      "Source chunk preview: they made a resear ch recommendation t o test t he benefit s of t his combination in childr en. \n",
      "How the r ecommenda tion mig ht affect practice \n",
      "The  ...\n",
      "------------------------------------------------------------\n",
      "Question: What are the recommendations in the NICE guidelines for managing hypertension in women of childbearing potential, particularly regarding treatment during pregnancy and breastfeeding?\n",
      "Answer A: The NICE guidelines recommend that women of childbearing potential with diagnosed hypertension should be offered antihypertensive drug treatment in line with the recommendations in the guidelines. For women considering pregnancy or who are pregnant or breastfeeding, hypertension should be managed according to the recommendations on the management of pregnancy with chronic hypertension and on antihypertensive treatment while breastfeeding.\n",
      "Source chunk preview: related clarification on br eastf eeding . \n",
      "1.4.26 For guidance on choice of antih yper tensiv e medicine in people wit h chr onic \n",
      "kidne y disease, s ...\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def answer_generated_questions(question_tuples, k, index, chunks, embedder, client):\n",
    "    results = []\n",
    "    for chunk, question in question_tuples:\n",
    "        answer = answer_query(question, k, index, chunks, embedder, client)\n",
    "        results.append({\n",
    "            \"chunk\": chunk,\n",
    "            \"question\": question,\n",
    "            \"answer\": answer\n",
    "        })\n",
    "    return results\n",
    "\n",
    "results_a = answer_generated_questions(\n",
    "    questions,\n",
    "    k=5,\n",
    "    index=index_a,\n",
    "    chunks=chunks_a,\n",
    "    embedder=embedder_a,\n",
    "    client=openai_client,\n",
    ")\n",
    "\n",
    "for item in results_a:\n",
    "    print(\"Question:\", item[\"question\"])\n",
    "    print(\"Answer A:\", item[\"answer\"])\n",
    "    print(\"Source chunk preview:\", item[\"chunk\"][:150], \"...\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec212b7",
   "metadata": {},
   "source": [
    "### Optional extension: add RAG B and create a comparison table\n",
    "\n",
    "If you implemented a second configuration (e.g. different chunking + OpenAI embeddings):\n",
    "\n",
    "1. Build `index_b` and `embedder_b`.\n",
    "2. Run `results_b = answer_generated_questions(..., index_b, chunks_b, embedder_b, client)`.\n",
    "3. For each question, compare:\n",
    "   - Which answer is more complete / specific?\n",
    "   - Which one is better grounded in the source chunk?\n",
    "4. Summarise your findings in a short **markdown cell** or a small table.\n",
    "\n",
    "---\n",
    "\n",
    "This concludes the core RAG exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "faa74961",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embedder_b' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m results_b = answer_generated_questions(\n\u001b[32m      2\u001b[39m     questions,\n\u001b[32m      3\u001b[39m     k=\u001b[32m5\u001b[39m,\n\u001b[32m      4\u001b[39m     index=index_b,\n\u001b[32m      5\u001b[39m     chunks=chunks_b,\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     embedder=\u001b[43membedder_b\u001b[49m,\n\u001b[32m      7\u001b[39m     client=openai_client,\n\u001b[32m      8\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'embedder_b' is not defined"
     ]
    }
   ],
   "source": [
    "results_b = answer_generated_questions(\n",
    "    questions,\n",
    "    k=5,\n",
    "    index=index_b,\n",
    "    chunks=chunks_b,\n",
    "    embedder=embedder_b,\n",
    "    client=openai_client,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc18c46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "\n",
    "for qa_a, qa_b in zip(results_a, results_b):\n",
    "    rows.append({\n",
    "        \"Question\": qa_a[\"question\"],\n",
    "        \"Answer A (BGE + config A)\": qa_a[\"answer\"],\n",
    "        \"Answer B (OpenAI + config B)\": qa_b[\"answer\"],\n",
    "        \"Source chunk (A) preview\": qa_a[\"chunk\"][:200] + \"...\"\n",
    "    })\n",
    "\n",
    "df_comparison = pd.DataFrame(rows)\n",
    "df_comparison"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
