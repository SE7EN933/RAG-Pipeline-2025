{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bad32605",
   "metadata": {},
   "source": [
    "# RAG Pipeline Exercise\n",
    "\n",
    "In this exercise you will build and **compare two simple Retrieval-Augmented Generation (RAG) pipelines**.\n",
    "\n",
    "You will work with a small collection of PDF documents (e.g. medical guidelines) and:\n",
    "\n",
    "1. Load and chunk the PDF documents.\n",
    "2. Create a vector index using **embedding model A** (local `BAAI/bge-m3`).\n",
    "3. Create a second index using **embedding model B** (e.g. OpenAI or Gemini embeddings).\n",
    "4. Implement a simple **retriever** and an **answering function** that calls an LLM with retrieved context.\n",
    "5. Automatically **generate questions** from the documents and use them to **compare two RAG configurations**.\n",
    "\n",
    "Cells marked with `# TODO` are **for students to implement**.\n",
    "Everything else is provided scaffolding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddf82e6",
   "metadata": {},
   "source": [
    "## 0. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be93ec53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# TODO (easy): skim the imports and make sure you understand what each library is used for.\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import glob\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# LLM / API clients (we will mainly use OpenAI here; Gemini can be added as a bonus)\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68f16980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load API keys from .env (you need to create this file once and add your keys)\n",
    "load_dotenv()\n",
    "\n",
    "deepinfra_key = os.getenv(\"DEEPINFRA_API_KEY\")\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "anthropic_api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "\n",
    "# For this exercise we mainly use OpenAI for both embeddings (RAG B) and chat completions.\n",
    "assert openai_api_key is not None, \"Please set OPENAI_API_KEY in your .env file.\"\n",
    "openai_client = OpenAI(api_key= deepinfra_key, base_url=\"https://api.deepinfra.com/v1/openai\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23ebcf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make pandas show the full table and full cell content\n",
    "pd.set_option(\"display.max_rows\", None)       # show all rows\n",
    "pd.set_option(\"display.max_columns\", None)    # show all columns\n",
    "pd.set_option(\"display.max_colwidth\", None)   # don't truncate cell text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba2719d",
   "metadata": {},
   "source": [
    "## 1. Load PDF documents\n",
    "\n",
    "We assume there is a `data/` folder containing one or more PDF files.\n",
    "\n",
    "**Task:** implement `load_pdfs(glob_path)` so that it:\n",
    "- Iterates over all PDF files matching `glob_path`\n",
    "- Reads them with `PdfReader`\n",
    "- Concatenates the text of all pages into **one long string**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8abcb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdfs(glob_path: str = \"data/*.pdf\") -> str:\n",
    "    \"\"\"Load all PDFs matching the pattern and return their combined text.\n",
    "\n",
    "    TODO:\n",
    "    - Use `glob.glob(glob_path)` to iterate over file paths\n",
    "    - For each file, open it in binary mode and create a `PdfReader`\n",
    "    - Loop over `reader.pages` and extract text with the extract_text() function\n",
    "    - Concatenate everything into a single string `text`\n",
    "    - Be robust: skip pages where `extract_text()` returns None\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    text = \"\"\n",
    "    for pdf_path in glob.glob(glob_path):\n",
    "        print(pdf_path)\n",
    "        with open(pdf_path, \"rb\") as f:\n",
    "            reader = PdfReader(f)\n",
    "            for page in reader.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    text += \" \" + page_text\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "416c3ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/Cipralex.pdf\n",
      "data/Candesartan.pdf\n",
      "data/Aspirin.pdf\n",
      "Number of characters: 139319\n",
      "Preview:  Cipralex®\n",
      "Lundbeck (Schweiz) AG\n",
      "Zusammensetzung\n",
      "Wirkstoffe\n",
      "Filmtabletten, Tropfen zum Einnehmen, Lösung:  Escitalopramum ut escitaloprami oxalas\n",
      "Hilfsstoffe\n",
      "Filmtabletten:  Cellulosum microcristallinum silicificatum, Talcum, Carmellosum natricum conexum enthält ungefähr 0,32 mg (10 mg) oder 0,63 mg (20 mg)\n",
      "natrium, Magnesii stearas, Hypromellosum, Macrogolum 400, E171\n",
      "Tropfen zum Einnehmen, Lösung (20 mg/ml):  Acidum Citricum, Ethanolum 96 per centum 100 mg pro ml, Natrii hydroxidum corresp. ma\n"
     ]
    }
   ],
   "source": [
    "# Run once and inspect\n",
    "raw_text = load_pdfs(\"data/*.pdf\")\n",
    "print(\"Number of characters:\", len(raw_text))\n",
    "print(\"Preview:\", raw_text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0c5a14",
   "metadata": {},
   "source": [
    "## 2. Chunk the text\n",
    "\n",
    "We will split the long text into overlapping chunks.\n",
    "\n",
    "Later you can **experiment** with different `chunk_size` and `chunk_overlap` to see how it affects retrieval.\n",
    "\n",
    "**Task:** start with the given parameters, run once, then try at least one alternative configuration and note the effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4e3f802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG A: 79 chunks produced, first chunk length = 1908\n"
     ]
    }
   ],
   "source": [
    "# Base configuration (RAG A)\n",
    "chunk_size_a = 2000\n",
    "chunk_overlap_a = 200\n",
    "\n",
    "splitter_a = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size_a,\n",
    "    chunk_overlap=chunk_overlap_a\n",
    ")\n",
    "\n",
    "chunks_a = splitter_a.split_text(raw_text)\n",
    "print(f\"RAG A: {len(chunks_a)} chunks produced, first chunk length = {len(chunks_a[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3511f4",
   "metadata": {},
   "source": [
    "## 3. Create embeddings and a FAISS index\n",
    "\n",
    "We start with **Embedding model A: `BAAI/bge-small-en`** using `sentence-transformers`.\n",
    "\n",
    "Then, as an optional extension, you can build **Embedding model B** using OpenAI or Gemini and compare.\n",
    "\n",
    "To keep the exercise manageable, the base version only **requires** BGE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b519161f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dimensionality (A): 768\n",
      "FAISS index (A) size: 79\n"
     ]
    }
   ],
   "source": [
    "# Embedding model A (local)\n",
    "model_name_a = \"intfloat/e5-base-v2\"\n",
    "embedder_a = SentenceTransformer(model_name_a)\n",
    "\n",
    "\n",
    "chunks_with_prefix = [\"passage: \" + chunk for chunk in chunks_a] #higher quality for this model\n",
    "# Compute embeddings for all chunks of configuration A\n",
    "embeddings_a = embedder_a.encode(chunks_with_prefix, convert_to_numpy=True)\n",
    "\n",
    "dimensions_a = embeddings_a.shape[1]\n",
    "print(\"Embedding dimensionality (A):\", dimensions_a)\n",
    "\n",
    "index_a = faiss.IndexFlatL2(dimensions_a)\n",
    "index_a.add(embeddings_a)\n",
    "print(\"FAISS index (A) size:\", index_a.ntotal)\n",
    "\n",
    "# Persist index/chunks if you like (optional)\n",
    "os.makedirs(\"faiss\", exist_ok=True)\n",
    "faiss.write_index(index_a, \"faiss/faiss_index_a.index\")\n",
    "with open(\"faiss/chunks_a.pkl\", \"wb\") as f:\n",
    "    pickle.dump(chunks_a, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0339224e",
   "metadata": {},
   "source": [
    "## 4. Implement a simple retriever\n",
    "\n",
    "We now implement a generic retrieval function that:\n",
    "1. Embeds the query.\n",
    "2. Searches the FAISS index.\n",
    "3. Returns the corresponding text chunks.\n",
    "\n",
    "We implement it for configuration A. If you built configuration B, you can reuse the same function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa6fbf82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of retrieved chunks: 3\n",
      "Preview of first chunk: Cipralex®\n",
      "Lundbeck (Schweiz) AG\n",
      "Zusammensetzung\n",
      "Wirkstoffe\n",
      "Filmtabletten, Tropfen zum Einnehmen, Lösung:  Escitalopramum ut escitaloprami oxalas\n",
      "Hilfsstoffe\n",
      "Filmtabletten:  Cellulosum microcristallinum silicificatum, Talcum, Carmellosum natricum conexum enthält ungefähr 0,32 mg (10 mg) oder 0,63 mg (20 mg)\n",
      "natrium, Magnesii stearas, Hypromellosum, Macrogolum 400, E171\n",
      "Tropfen zum Einnehmen, Lösung\n"
     ]
    }
   ],
   "source": [
    "def retrieve_texts(query: str, k: int, index, chunks, embedder) -> list:\n",
    "    \"\"\"Return the top-k most similar chunks for a query.\n",
    "\n",
    "    TODO (students):\n",
    "    - Encode the query with `embedder.encode(...)`\n",
    "    - Call `index.search(query_embedding, k)`\n",
    "    - Use the returned indices to select the chunks\n",
    "    - Return a list of strings (chunks)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    query_with_prefix = \"query: \" + query #higher quality for this model\n",
    "    query_emb = embedder.encode([query_with_prefix], convert_to_numpy=True)\n",
    "    distances, indices = index.search(query_emb, k)\n",
    "    retrieved = [chunks[i] for i in indices[0]]\n",
    "    return retrieved\n",
    "\n",
    "# Quick sanity check\n",
    "test_query = \"Wie soll Cipralex gelagert werden?\"\n",
    "retrieved_text = retrieve_texts(test_query, k=3, index=index_a, chunks=chunks_a, embedder=embedder_a)\n",
    "print(\"Number of retrieved chunks:\", len(retrieved_text))\n",
    "print(\"Preview of first chunk:\", retrieved_text[0][:400])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f646fdc6",
   "metadata": {},
   "source": [
    "## 5. Implement `answer_query` using an LLM\n",
    "\n",
    "Now we build the actual RAG call:\n",
    "\n",
    "1. Use `retrieve_texts` to get top-`k` chunks.\n",
    "2. Concatenate them into a context string.\n",
    "3. Build a prompt that:\n",
    "   - shows the context\n",
    "   - asks the model to answer the user question based **only** on this context.\n",
    "4. Call the OpenAI chat completion API.\n",
    "\n",
    "This is the **core RAG function**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d94610c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG answer: Cipralex soll in der Originalverpackung und nicht über 30°C gelagert werden. Es soll außerdem außer Reichweite von Kindern aufbewahrt werden.\n"
     ]
    }
   ],
   "source": [
    "def answer_query_a(query: str, k: int, index, chunks, embedder, client: OpenAI) -> str:\n",
    "    \"\"\"RAG-style answer: retrieve context and ask an LLM.\n",
    "\n",
    "    TODO (students):\n",
    "    - Use `retrieve_texts` to get `k` relevant chunks.\n",
    "    - Join them into a single context string.\n",
    "    - Build a chat prompt that instructs the model to answer *only* using the context.\n",
    "    - Call `client.chat.completions.create(...)` with model `\"gpt-4o-mini\"` (or similar).\n",
    "    - Return the model's answer text.\n",
    "    \"\"\"\n",
    "    retrieved_chunks = retrieve_texts(query, k, index, chunks, embedder)\n",
    "    context = \"\\n\\n---\\n\\n\".join(retrieved_chunks)\n",
    "\n",
    "    system_prompt = (\n",
    "        \"\"\"Du bist ein hilfreicher Assistent, der Fragen NUR basierend auf dem bereitgestellten Kontext beantwortet.\n",
    "  Wenn die Antwort nicht im Kontext enthalten ist, sage dass du es nicht weisst.\n",
    "  Antworte auf Deutsch.\"\"\"\n",
    "    )\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": f\"Kontext:\\n{context}\\n\\nFrage: {query}\"}\n",
    "    ]\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"meta-llama/Llama-3.3-70B-Instruct-Turbo\",\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message.content.strip()\n",
    "\n",
    "# Quick manual test\n",
    "answer = answer_query_a(test_query, k=3, index=index_a, chunks=chunks_a, embedder=embedder_a, client=openai_client)\n",
    "print(\"RAG answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a10c3a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frage 1: Wie wird Candesartan normalerweise bei Erwachsenen dosiert?\n",
      "Antwort: Ich weiß es nicht. Der Kontext liefert Informationen über die Pharmakokinetik von Candesartan, insbesondere bei Patienten mit eingeschränkter Nierenfunktion und älteren Patienten, aber es gibt keine spezifischen Angaben über die Standarddosierung von Candesartan bei Erwachsenen.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Frage 2: Wer sollte Aspirin nicht anwenden? Nenne mindestens eine wichtige Gegenanzeige.\n",
      "Antwort: Aspirin sollte nicht von Patienten mit Asthma bronchiale oder allgemeiner Neigung zu Überempfindlichkeit angewendet werden, da Acetylsalicylsäure Bronchospasmen begünstigen und Asthmaanfälle oder andere Überempfindlichkeitsreaktionen auslösen kann. Ein weiterer wichtiger Punkt ist, dass Patienten mit Glucose-6-Phosphatdehydrogenase (G6PD)-Mangel Aspirin nicht anwenden sollten, da es eine Hämolyse oder hämolytische Anämie induzieren könnte. Darüber hinaus sollten Kinder und Jugendliche unter 18 Jahren Aspirin bei Fieber und/oder viralen Erkrankungen nur auf ärztliche Verschreibung und nur als Mittel der zweiten Wahl einnehmen, wegen des möglichen Auftretens des Reye-Syndroms.\n"
     ]
    }
   ],
   "source": [
    "# Testfrage 1\n",
    "frage1 = \"Wie wird Candesartan normalerweise bei Erwachsenen dosiert?\"\n",
    "antwort1 = answer_query_a(frage1, k=3, index=index_a, chunks=chunks_a, embedder=embedder_a, client=openai_client)\n",
    "print(\"Frage 1:\", frage1)\n",
    "print(\"Antwort:\", antwort1)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Testfrage 2\n",
    "frage2 = \"Wer sollte Aspirin nicht anwenden? Nenne mindestens eine wichtige Gegenanzeige.\"\n",
    "antwort2 =  answer_query_a(frage2, k=3, index=index_a, chunks=chunks_a, embedder=embedder_a, client=openai_client)\n",
    "print(\"Frage 2:\", frage2)\n",
    "print(\"Antwort:\", antwort2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
